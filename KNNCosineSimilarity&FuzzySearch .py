# -*- coding: utf-8 -*-
"""PreProcessingŸêŸêKNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hkEU6Ey4dXuGNCI9GTk0ku4foy8rysbF
"""

import pandas as pd
from google.colab import drive
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import plotly.express as px
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder,StandardScaler,RobustScaler,MinMaxScaler, OneHotEncoder, OrdinalEncoder
from sklearn.compose import ColumnTransformer
from sklearn.decomposition import PCA
from sklearn.preprocessing import OneHotEncoder
from sklearn.cluster import KMeans, DBSCAN
from sklearn.neighbors import NearestNeighbors
from sklearn.metrics import mean_squared_error, accuracy_score,silhouette_score, davies_bouldin_score
import scipy.stats as stats

"""# Data Preprocssing and analysis

"""

# from google.colab import drive
# drive.mount('/content/drive')

drive.mount('/content/drive')
df = pd.read_csv('/content/drive/MyDrive/AqurHunt.csv')
df

# configures the pandas library to display all columns of a DataFrame

pd.set_option("display.max_columns", None)

print(df.info())

print(df.describe())

#check for if we have missing values
total_missing_values = df.isnull().sum()
total_missing_values

# Drop rows where 'bathrooms' is explicitly 'None'
df = df[df['bathrooms'] != 'None']

# Convert 'bathrooms' and 'bedrooms' to numeric, handling errors
df['bathrooms'] = pd.to_numeric(df['bathrooms'], errors='coerce').astype('Int64')
df['bedrooms'] = pd.to_numeric(df['bedrooms'], errors='coerce').astype('Int64')

# Now drop NaN values that resulted from conversion
df = df.dropna(subset=['bedrooms', 'bathrooms'])

# Fill NaN values in 'down_payment_price' with 0
df['down_payment_price'] = df['down_payment_price'].fillna(0)

# Fill NaN values in 'district/compound' with 'unknown'
df['district/compound'] = df['district/compound'].fillna('unknown')

# Ensure 'furnished' column values are consistent for comparison
df['furnished'] = df['furnished'].astype(str).str.strip().str.capitalize()

# Update 'completion_status' based on 'furnished' values
df.loc[df['completion_status'].isna() & df['furnished'].isin(['Yes', 'Partly']), 'completion_status'] = 'Completed'
df.loc[df['completion_status'].isna() & df['furnished'].isin(['No']), 'completion_status'] = 'Unknown'

print(df.info())

#check for if we have missing values
total_missing_values = df.isnull().sum()
total_missing_values

# Convert 'listed_date' to datetime (handling UTC 'Z')
df['listed_date'] = pd.to_datetime(df['listed_date'], format="%Y-%m-%dT%H:%M:%SZ")

# Check for duplicates
duplicates = df.duplicated()
print(f"Number of duplicate rows: {duplicates.sum()}")

# Remove duplicates and keep the first occurrence
df = df.drop_duplicates()

# Reset index after removing duplicates (optional)
df.reset_index(drop=True, inplace=True)

#Ensure consistent capitalization in categorical columns
categorical_columns = df.select_dtypes(include='object').columns
for column in categorical_columns:
    df[column] = df[column].str.capitalize()

df.describe(include='object')

#The values for each categorical columns
unique_values = {}

for column in categorical_columns:
    unique_values[column] = df[column].unique()
    print(f"Unique values in '{column}':")
    print(unique_values[column])
    print()
#print(unique_values)

# To know the range of each category
# Group by price_category and find min and max prices
result = df.groupby('offering_type')['price'].agg(['min', 'max']).reset_index()
print(result)

#droping irrelvent columns
df.drop(columns=['id'], inplace=True)
df.drop(columns=['url+X13A1:V13'], inplace=True)
df.drop(columns=['location_full_name'], inplace=True)
df.drop(columns=['has_view_360'], inplace=True)
df.drop(columns=['amenity_names'], inplace=True)
df.drop(columns=['payment_method'], inplace=True)
df.drop(columns=['listed_date'], inplace=True)

df.info()

df.select_dtypes(include = 'object').columns

df.dtypes

"""## **1. visualization**"""

# Preparing data by filtering extreme values
filtered_data = df[df['price'] < df['price'].quantile(0.95)]

plt.figure(figsize=(12, 8))
ax = sns.histplot(filtered_data['price'], bins=30, kde=True, color='blue', edgecolor='black', linewidth=1.5)
plt.title('Property Prices Distribution', fontsize=16)
plt.xlabel('Price (EGP)', fontsize=14)
plt.ylabel('Frequency', fontsize=14)
plt.axvline(filtered_data['price'].mean(), color='red', linestyle='--', label=f'Mean Price: {filtered_data["price"].mean():,.2f} EGP')
plt.legend()

# Annotate each bar with the count
for p in ax.patches:
    ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='bottom', fontsize=10, color='black')

plt.show()

plt.figure(figsize=(12, 5))
ax_type = sns.countplot(data=df, x='property_type', palette='viridis')
plt.title('Distribution of Property Types')
plt.xlabel("Property Type")
plt.ylabel("Count")
plt.xticks(rotation=45)
for p in ax_type.patches:
    ax_type.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),
                     ha='center', va='center', fontsize=12, color='black')
plt.show()

# Calculate the median price for each apartment type
median_prices = df.groupby('property_type')['price'].median()

# Create the bar chart
plt.figure(figsize=(12, 6))
ax_bar = sns.barplot(x=median_prices.index, y=median_prices.values, palette='viridis')

# Title and labels
plt.title('Price Distribution by Apartment Type')
plt.xlabel("Apartment Type")
plt.ylabel("Median Price (EGP)")

# Rotate x-axis labels
plt.xticks(rotation=45)

# Add annotations with the median value
for i, median in enumerate(median_prices.values):
    ax_bar.annotate(f'{median:.0f}',
                    (i, median),
                    ha='center', va='center', fontsize=12, color='black')

# Show the plot
plt.show()

plt.figure(figsize=(10, 6))
ax_city = sns.countplot(data=df, x='city', palette='icefire')
plt.title('Distribution of Properties by City')
plt.xlabel("City Name")
plt.ylabel("Count")
plt.xticks(rotation=45)
for p in ax_city.patches:
    ax_city.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),
                     ha='center', va='center', fontsize=10, color='black')
plt.show()

# Bar chart for 'furnishing'
plt.figure(figsize=(10, 6))
ax_furnishing = sns.countplot(data=df, x='furnished', palette='crest')
plt.title('Furnishing Status of Properties')
plt.xlabel("Property Furnishing")
plt.ylabel("Count")
for p in ax_furnishing.patches:
    ax_furnishing.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),
                           ha='center', va='center', fontsize=10, color='black')
plt.show()

# Bar chart for 'completion_status'
plt.figure(figsize=(10, 6))
ax_completion = sns.countplot(data=df, x='completion_status', palette='viridis')
plt.title('Completion Status of Properties')
plt.xlabel("Completion Status")
plt.ylabel("Count")
for p in ax_completion.patches:
    ax_completion.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),
                           ha='center', va='center', fontsize=10, color='black')
plt.show()

fig = px.scatter_mapbox(df, lat='lat', lon='lon',
                        color_continuous_scale=px.colors.cyclical.IceFire, size_max=15, zoom=7,
                        hover_data={
                            'price': True,
                            'property_type': True,
                            'furnished': True,
                            'completion_status': True,
                            'town': True,
                            'city': True,
                            'district/compound': True
                        },
                        mapbox_style="carto-positron")

fig.update_layout(mapbox_style="open-street-map")
fig.update_layout(margin={"r":0,"t":0,"l":0,"b":0})
fig.show()

# Ensure only numeric columns are included
numeric_df = df.select_dtypes(include=[np.number])

# Calculating the correlation matrix
correlation_matrix = numeric_df.corr()

# Visualizing the correlation matrix using a heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()



"""# Recommendtion Model Training

> Add blockquote

## Prep Data
"""

dataset = df.copy()
dataset

categorical_cols = ['property_type', 'city', 'town', 'district/compound', 'offering_type', 'furnished']
numerical_cols = ['price', 'lat', 'lon', 'bedrooms', 'bathrooms', 'size', 'down_payment_price']

def extract_data(dataframe, filters=None):
    extracted = dataframe.copy()

    if filters:
        for col, val in filters.items():
            extracted = extracted[extracted[col] == val]

    return extracted.reset_index(drop=True)

#code categorical data and store mappings
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col].astype(str))
    label_encoders[col] = le

# Normalize numerical data
scaler = MinMaxScaler()
df[numerical_cols] = scaler.fit_transform(df[numerical_cols])

# Combine encoded categorical and standardized numerical data
prep_data = np.hstack([df[numerical_cols].to_numpy(), df[categorical_cols].to_numpy()])
prep_data

"""Fitting the Model

"""

from sklearn.neighbors import NearestNeighbors

model = NearestNeighbors(metric='cosine', algorithm='brute')
model.fit(prep_data)

"""## End To End Functions

"""

def recommendEncoded(index, n_neighbors=6):
    input_vector = prep_data[index].reshape(1, -1)
    distances, indices = model.kneighbors(input_vector, n_neighbors=n_neighbors)

    # Get original rows from the real (non-encoded) dataset
    return dataset.iloc[indices[0]]

recommendations = recommendEncoded(index=7)
recommendations[['property_type', 'city', 'price', 'size', 'bedrooms', 'bathrooms']]

def recommend(index, n_neighbors=6):
    input_vector = prep_data[index].reshape(1, -1)
    distances, indices = model.kneighbors(input_vector, n_neighbors=n_neighbors)
    return indices[0]  # just return the indices

indices = recommend(index=100)

# Get raw original rows
recommendations = dataset.iloc[indices].copy()
recommendations

"""# Search Engine

# Filtering-Based
"""

import pandas as pd

def dynamic_filter(df):
    print("üîç Real Estate Search - You can skip any field by pressing Enter")

    # Step 1: Collect available options from the dataset
    cities = df['city'].dropna().unique().tolist()
    towns = df['town'].dropna().unique().tolist()
    compounds = df['district/compound'].dropna().unique().tolist()

    # Step 2: Ask for inputs (optional)
    city = input(f"\nSelect a City ({len(cities)} options) or press Enter to skip: ").strip()
    if city not in cities:
        city = None

    town = input(f"\nSelect a Town ({len(towns)} options) or press Enter to skip: ").strip()
    if town not in towns:
        town = None

    compound = input(f"\nSelect a Compound/District ({len(compounds)} options) or press Enter to skip: ").strip()
    if compound not in compounds:
        compound = None

    try:
        min_price = input("\nMinimum Price (or press Enter to skip): ").strip()
        min_price = int(min_price) if min_price else None
    except:
        min_price = None

    try:
        max_price = input("Maximum Price (or press Enter to skip): ").strip()
        max_price = int(max_price) if max_price else None
    except:
        max_price = None

    try:
        bedrooms = input("\nMinimum Number of Bedrooms (or press Enter to skip): ").strip()
        bedrooms = int(bedrooms) if bedrooms else None
    except:
        bedrooms = None

    try:
        bathrooms = input("Minimum Number of Bathrooms (or press Enter to skip): ").strip()
        bathrooms = int(bathrooms) if bathrooms else None
    except:
        bathrooms = None

    # Step 3: Apply filters dynamically
    filtered = df.copy()

    if city:
        filtered = filtered[filtered['city'] == city]

    if town:
        filtered = filtered[filtered['town'] == town]

    if compound:
        filtered = filtered[filtered['district/compound'] == compound]

    if min_price:
        filtered = filtered[filtered['price'] >= min_price]

    if max_price:
        filtered = filtered[filtered['price'] <= max_price]

    if bedrooms:
        filtered = filtered[filtered['bedrooms'] >= bedrooms]

    if bathrooms:
        filtered = filtered[filtered['bathrooms'] >= bathrooms]

    # Final check
    if filtered.empty:
        print("\n‚ùå No properties match the selected criteria.")
        return pd.DataFrame()  # return empty to avoid crashing KNN
    else:
        print(f"\n‚úÖ Found {len(filtered)} matching properties.")
        return filtered

filtered_data = dynamic_filter(dataset)

if not filtered_data.empty:
    # Send this to your KNN model
    print("\nYou can now apply KNN on the filtered data.")
filtered_data

dataset = dataset.copy()
filtered_data = filtered_data.copy()

# Get index positions in dataset
filtered_indices = dataset.apply(tuple, axis=1).isin(filtered_data.apply(tuple, axis=1))
filtered_indices = list(filtered_indices[filtered_indices].index)

# Use integer slicing directly
prep_data_filtered = prep_data[filtered_indices]

filtered_knn_model = NearestNeighbors(metric='cosine', algorithm='brute')
filtered_knn_model.fit(prep_data_filtered)

# Choose a sample input within the filtered dataset (e.g., first row)
selected_index = 0  # This is relative to the filtered set

input_vector = prep_data_filtered[selected_index].reshape(1, -1)

# Get nearest neighbors from the filtered KNN model
distances, indices = filtered_knn_model.kneighbors(input_vector, n_neighbors=5)

# Map back to full dataset using filtered_indices
result_indices = [filtered_indices[i] for i in indices[0]]

# Show recommended rows from the original dataset (human-readable)
recommended_properties = dataset.iloc[result_indices]

recommended_properties

"""# fuzzy matching

"""

!pip install rapidfuzz

from rapidfuzz import process

def fuzzy_match(user_input, choices, threshold=80):
    """
    Matches user input against a list of choices using RapidFuzz.
    Returns the best match if the similarity score exceeds the threshold.
    """
    if not user_input:
        return None

    match = process.extractOne(user_input, choices, score_cutoff=threshold)
    if match:
        return match[0]
    return None

def fuzzy_filter(df):
    print("üîç Real Estate Search ‚Äî Use fuzzy search. Press Enter to skip any input.")

    # Unique values from data
    cities = df['city'].dropna().unique().tolist()
    towns = df['town'].dropna().unique().tolist()
    compounds = df['district/compound'].dropna().unique().tolist()

    # User inputs
    city_input = input("\nEnter city (or press Enter to skip): ").strip()
    matched_city = fuzzy_match(city_input, cities)

    town_input = input("\nEnter town (or press Enter to skip): ").strip()
    matched_town = fuzzy_match(town_input, towns)

    compound_input = input("\nEnter district/compound (or press Enter to skip): ").strip()
    matched_compound = fuzzy_match(compound_input, compounds)

    try:
        min_price = int(input("\nMinimum price (or Enter to skip): ").strip() or 0)
    except:
        min_price = None

    try:
        max_price = int(input("Maximum price (or Enter to skip): ").strip() or 0)
    except:
        max_price = None

    try:
        bedrooms = int(input("\nMinimum bedrooms (or Enter to skip): ").strip() or 0)
    except:
        bedrooms = None

    try:
        bathrooms = int(input("Minimum bathrooms (or Enter to skip): ").strip() or 0)
    except:
        bathrooms = None

    # Begin filtering
    filtered = df.copy()

    if matched_city:
        filtered = filtered[filtered['city'] == matched_city]

    if matched_town:
        filtered = filtered[filtered['town'] == matched_town]

    if matched_compound:
        filtered = filtered[filtered['district/compound'] == matched_compound]

    if min_price:
        filtered = filtered[filtered['price'] >= min_price]

    if max_price:
        filtered = filtered[filtered['price'] <= max_price]

    if bedrooms:
        filtered = filtered[filtered['bedrooms'] >= bedrooms]

    if bathrooms:
        filtered = filtered[filtered['bathrooms'] >= bathrooms]

    if filtered.empty:
        print("\n‚ùå No matching properties found.")
    else:
        print(f"\n‚úÖ {len(filtered)} properties found based on your fuzzy criteria.")
    return filtered

from sklearn.neighbors import NearestNeighbors

def recommend_from_filtered_selection(dataset, prep_data, filtered_df, selected_index=0, n_neighbors=5):
    """
    Takes filtered original DataFrame (filtered_df) and uses the corresponding
    encoded data (prep_data) to apply KNN and return top recommendations.

    Parameters:
    - dataset: original unencoded DataFrame
    - prep_data: NumPy array or encoded DataFrame (same row order as dataset)
    - filtered_df: filtered DataFrame from user input (subset of dataset)
    - selected_index: which row inside filtered_df to base recommendations on
    - n_neighbors: number of similar listings to return

    Returns:
    - DataFrame of recommended listings (from original dataset)
    """
    # Ensure dataset copies (no side effects)
    dataset = dataset.copy()
    filtered_df = filtered_df.copy()

    # Step 1: Match filtered_df rows to dataset to get their indices
    filtered_indices = dataset.apply(tuple, axis=1).isin(filtered_df.apply(tuple, axis=1))
    filtered_indices = list(filtered_indices[filtered_indices].index)

    if not filtered_indices:
        print("No matching records found for recommendation.")
        return pd.DataFrame()

    # Step 2: Slice prep_data using matched indices
    prep_data_filtered = prep_data[filtered_indices]

    # Step 3: Fit KNN on filtered encoded data
    knn_model = NearestNeighbors(metric='cosine', algorithm='brute')
    knn_model.fit(prep_data_filtered)

    # Step 4: Get user input vector inside filtered set
    input_vector = prep_data_filtered[selected_index].reshape(1, -1)

    # Step 5: Run KNN and return corresponding records from dataset
    distances, indices = knn_model.kneighbors(input_vector, n_neighbors=n_neighbors)
    recommended_indices = [filtered_indices[i] for i in indices[0]]

    return dataset.iloc[recommended_indices]

filtered_df = fuzzy_filter(dataset)

if not filtered_df.empty:
    # Continue to KNN step using previously built logic
    recommended = recommend_from_filtered_selection(dataset, prep_data, filtered_df)


recommended

"""# **Ensemble**"""

categorical_features = ['property_type', 'city', 'town', 'district/compound', 'completion_status', 'offering_type', 'furnished']
numerical_features = ['lat', 'lon', 'bedrooms', 'bathrooms', 'size', 'down_payment_price']

# Define preprocessing steps
numerical_transformer = Pipeline(steps=[
    ('scaler', StandardScaler()) # Or any other scaler like MinMaxScaler, RobustScaler
])

categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore')) # handle_unknown='ignore' is useful for unseen categories during prediction
])

# Create preprocessor object using ColumnTransformer
column_trans = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ],
    remainder='passthrough' # Keep other columns (if any)
)

# Define features (X) and target (y)
X = df.drop('price', axis=1)  # Assuming 'price' is your target variable
y = df['price']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define base estimator
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import BaggingRegressor
from sklearn.pipeline import Pipeline


base_estimator = DecisionTreeRegressor(max_depth=None, min_samples_leaf=2, random_state=42)


bag = BaggingRegressor(
        estimator      = base_estimator,
        n_estimators   = 500,      # large ‚àöN rule of thumb
        max_samples    = 0.8,      # bootstrap rows
        max_features   = 1.0,      # all columns per tree
        bootstrap      = True,
        oob_score      = True,     # free validation!
        n_jobs         = -1,
        random_state   = 42
)
model = Pipeline([('prep', column_trans), # Use column_trans for preprocessing
                  ('bag',  bag)])
model.fit(X_train, y_train)
print('OOB R¬≤:', model[-1].oob_score_)          # quick sanity check



categorical_features = ['property_type', 'city', 'town', 'district/compound', 'completion_status', 'offering_type', 'furnished']
numerical_features = ['lat', 'lon', 'bedrooms', 'bathrooms', 'size', 'down_payment_price']

# Define preprocessing steps
numerical_transformer = Pipeline(steps=[
    ('scaler', StandardScaler()) # Or any other scaler like MinMaxScaler, RobustScaler
])

categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore')) # handle_unknown='ignore' is useful for unseen categories during prediction
])

# Create preprocessor object using ColumnTransformer
column_trans = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ],
    remainder='passthrough' # Keep other columns (if any)
)

# Define features (X) and target (y)
X = df.drop('price', axis=1)  # Assuming 'price' is your target variable
y = df['price']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)